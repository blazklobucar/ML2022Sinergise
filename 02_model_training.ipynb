{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sinergise Summer School 2022\n",
    "\n",
    "## EO Research Team ML part I\n",
    "\n",
    "\n",
    "Welcome! This notebook will show you how to get your hands dirty with machine learning (ML). We will look at the basics of how to use the distribution properties to classify burned areas, and proceed to using existing ML tools to improve our classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Decision Trees Intro\n",
    "\n",
    "Decision trees are one of the simplest (and most explainable) machine learning models.  The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n",
    "\n",
    "Some advantages of decision trees are:\n",
    "* simple to understand and to interpret (can be visualized)\n",
    "* requires little data preparation (other techniques often require normalization, imputation)\n",
    "* the cost of using DTs (i.e., predicting data) is logarithmic in the number of data points used to train the tree\n",
    "\n",
    "The disadvantages of decision trees include:\n",
    "* can be easy to overfit\n",
    "* can be problematic on unbalanced data\n",
    "\n",
    "![](https://regenerativetoday.com/wp-content/uploads/2022/04/dt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Homecooked decision tree\n",
    "\n",
    "Let's try to use \"brain learning\" first. \n",
    "\n",
    "_Can we make our own decision tree using only one or two features?_\n",
    "\n",
    "It's oftentimes surprising how good simple models can be and it also gives us a nice baseline to compare our machine learning model to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# use the same dataset as in Part I\n",
    "df_path = \"s3://eo-learn.sentinel-hub.com/workshops_data/summer_school_sinergise_2022/input_data/wildfires_single_scene_training_dataset.parquet\"\n",
    "df = pd.read_parquet(df_path)\n",
    "\n",
    "# show content\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "Below is an example skeleton decision tree. \n",
    "\n",
    "Create your own by looking at the band distributions from Part I and specifying regions for the particular class:\n",
    "- 0: Not burned\n",
    "- 1: Burned\n",
    "\n",
    "\n",
    "```python\n",
    "def homecooked_decision_tree(row):\n",
    "    if row.B08 <= 0.15:\n",
    "        return 1  # Burned\n",
    "    elif ...:\n",
    "        if ...:\n",
    "            ...\n",
    "    elif ...:\n",
    "        return 0 # Not burned\n",
    "    return 0 # Not burned\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: MAKE YOUR OWN DECISION TREE\n",
    "# No need to complicate, start with something very simple. You will be surprised at how good it can get.\n",
    "\n",
    "\n",
    "def homecooked_decision_tree(row):\n",
    "    if ...\n",
    "        return ...\n",
    "    elif ...:\n",
    "        return ...\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "How did we do? _We don't know yet!_ \n",
    "\n",
    "We need to evaluate the model by looking at the results and coming up with some qualitative and quantitative ways of evaluating them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qualitative assessment\n",
    "\n",
    "We've prepared a utility function which you can use to apply the homecooked algorithm and see the output on the EOPatches.\n",
    "\n",
    "Compare the output of the algorithm to the mask! What do you think about the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from eolearn.core import EOPatch\n",
    "from utils import plot_combo\n",
    "\n",
    "# download from AWS S3 bucket\n",
    "eop_path = \"s3://eo-learn.sentinel-hub.com/workshops_data/summer_school_sinergise_2022/input_data/eopatches/eopatch_1\"\n",
    "eop = EOPatch.load(eop_path, lazy_loading=True)\n",
    "\n",
    "# plot true color and reference mask\n",
    "rgb = np.concatenate([eop.data[\"B04\"], eop.data[\"B03\"], eop.data[\"B02\"]], axis=-1)\n",
    "plot_combo(rgb, eop.mask_timeless[\"BURN_AREA\"], brightness_factor=3.5, clip=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from utils import apply_function_to_eopatch, plot_mask\n",
    "\n",
    "# apply homecooked algo on eopatch\n",
    "eop = apply_function_to_eopatch(\n",
    "    eopatch=eop,\n",
    "    timestamp_index=1,\n",
    "    function=homecooked_decision_tree,\n",
    "    output_mask_name=\"HOMECOOKED_DECISION_TREE\",\n",
    ")\n",
    "\n",
    "# compare reference mask with our homecooked algorithm\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "plot_mask(eop.mask_timeless[\"BURN_AREA\"], ax=axs[0], title=\"Reference mask\")\n",
    "plot_mask(\n",
    "    eop.mask_timeless[\"HOMECOOKED_DECISION_TREE\"],\n",
    "    ax=axs[1],\n",
    "    title=\"Homecooked decision tree\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantitative assessment\n",
    "\n",
    "But how to best check this in a __quantitative__ way? \n",
    "\n",
    "We could compare how well our algorithm output `HOMECOOKED_DECISION_TREE` agrees with the reference labels `BURN_AREA`!\n",
    "\n",
    "This can be done with the _confusion matrix_, which shows the different prediction labels compared to the different reference labels.\n",
    "This way one can extract the amounts of true-positives, true-negatives, false-positives, and false-negatives.\n",
    "\n",
    "![cm](https://miro.medium.com/max/667/1*3yGLac6F4mTENnj5dBNvNQ.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# extract reference and predictions\n",
    "y_true = eop.mask_timeless[\"BURN_AREA\"].ravel()\n",
    "y_pred = eop.mask_timeless[\"HOMECOOKED_DECISION_TREE\"].ravel()\n",
    "cm_homecooked_eop = confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "# plot CM\n",
    "plot_confusion_matrix(\n",
    "    conf_mat=cm_homecooked_eop,\n",
    "    show_normed=True,\n",
    "    class_names=[\"Not burned\", \"Burned\"],\n",
    "    axis=ax,\n",
    ")\n",
    "ax.set_title(\"Homecooked Decision Trees CM - one EOPatch\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think about the performance of the homecooked model?\n",
    "\n",
    "How does it perform on the full training dataset? Do you understand the differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply it on the FULL dataset! using df.apply()\n",
    "df[\"HOMECOOKED_DECISION_TREE\"] = df.apply(...)\n",
    "\n",
    "y_true = df[\"BURN_AREA\"]\n",
    "y_pred = df[\"HOMECOOKED_DECISION_TREE\"]\n",
    "cm_homecooked = confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
    "cm_homecooked_norm = confusion_matrix(y_true=y_true, y_pred=y_pred, normalize=\"true\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "plot_confusion_matrix(\n",
    "    conf_mat=cm_homecooked,\n",
    "    show_normed=True,\n",
    "    class_names=[\"Not burned\", \"Burned\"],\n",
    "    axis=ax,\n",
    ")\n",
    "ax.set_title(\"Homecooked Decision Trees CM\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Evaluation Metrics\n",
    "\n",
    "F1 score, precision, accuracy, recall, (check out `sklearn.metrics`, which has a ton of metrics).\n",
    "\n",
    "What do they mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import other metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "print(f\"true-negative-rate: {cm_homecooked_norm[0,0]:.3f}\")\n",
    "print(f\"false-positive-rate: {cm_homecooked_norm[0,1]:.3f}\")\n",
    "print(f\"false-negative-rate: {cm_homecooked_norm[1,0]:.3f}\")\n",
    "print(f\"true-positive-rate: {cm_homecooked_norm[1,1]:.3f}\")\n",
    "\n",
    "print(f\"accuracy: {accuracy_score(y_true, y_pred):.3f}\")\n",
    "print(f\"precision: {precision_score(y_true, y_pred):.3f}\")\n",
    "print(f\"recall: {recall_score(y_true, y_pred):.3f}\")\n",
    "print(f\"F1-score: {f1_score(y_true, y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "Now let's try using official tools for Decision Trees classification. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split\n",
    "\n",
    "Before we proceed, **let's make a train/test split first.**\n",
    "\n",
    "This is needed in order not to overfit the model to the dataset. The simplest train/test split is to just randomly split the dataset. \n",
    "\n",
    "Let's use a 80/20 train/test split, since this method is the simplest\n",
    "\n",
    "_Can you think of a more appropriate method, considering we're dealing with geospatial data?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 0.8  # size of the training set (fraction)\n",
    "df[\"SPLIT\"] = ...\n",
    "\n",
    "df_train = df[df.SPLIT == \"TRAIN\"]\n",
    "df_test = df[df.SPLIT == \"TEST\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features to be considered\n",
    "feature_columns = [...]\n",
    "label_column = \"BURN_AREA\"\n",
    "\n",
    "# extract values\n",
    "features_train, labels_train = (\n",
    "    df_train[feature_columns].values,\n",
    "    df_train[label_column].values,\n",
    ")\n",
    "features_test, labels_test = (\n",
    "    df_test[feature_columns].values,\n",
    "    df_test[label_column].values,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "clf = clf.fit(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "# plot tree using the utility function\n",
    "tree.plot_tree(\n",
    "    clf,\n",
    "    filled=True,\n",
    "    ax=ax,\n",
    "    feature_names=feature_columns,\n",
    "    class_names=[\"Not burned\", \"Burned\"],\n",
    "    impurity=False,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Repeating processes from before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import apply_decision_trees_to_eopatch\n",
    "\n",
    "# copy-pasta for convenience\n",
    "clf = tree.DecisionTreeClassifier(max_depth=6, random_state=42)\n",
    "clf = clf.fit(features_train, labels_train)\n",
    "\n",
    "# apply decision trees on eopatch\n",
    "eop = apply_decision_trees_to_eopatch(\n",
    "    eopatch=eop,\n",
    "    timestamp_index=1,\n",
    "    classifier=clf,\n",
    "    output_mask_name=\"DECISION_TREE\",\n",
    "    features=feature_columns,\n",
    ")\n",
    "\n",
    "# compare reference mask with our homecooked algorithm\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "plot_mask(eop.mask_timeless[\"BURN_AREA\"], ax=axs[0], title=\"Reference mask\")\n",
    "plot_mask(\n",
    "    eop.mask_timeless[\"DECISION_TREE\"],\n",
    "    ax=axs[1],\n",
    "    title=\"Official decision tree\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DECISION_TREE\"] = ...\n",
    "\n",
    "y_true = df[\"BURN_AREA\"].values\n",
    "y_pred = df[\"DECISION_TREE\"].values\n",
    "cm_dt = confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
    "cm_dt_norm = confusion_matrix(y_true=y_true, y_pred=y_pred, normalize=\"true\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "plot_confusion_matrix(\n",
    "    conf_mat=cm_dt, show_normed=True, class_names=[\"Not burned\", \"Burned\"], axis=ax\n",
    ")\n",
    "ax.set_title(\"Decision Trees CM\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate F1-score for multiple depths (hyper-parameter optimization)\n",
    "depth_list = list(range(1, 7))\n",
    "scores = []\n",
    "for depth in depth_list:\n",
    "    # 1. define the classifier with dynamic depth\n",
    "    # 2. fit the classifier\n",
    "    # 3. predict on test features\n",
    "    # 4. calculate score on test\n",
    "\n",
    "    scores.append(...)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "ax.plot(depth_list, scores, \"C0x-\")\n",
    "ax.set_xlabel(\"Max Depth of DT\")\n",
    "ax.set_ylabel(\"F1 score\")\n",
    "ax.set_title(\"F1 score for different max depth of DT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## LGBM\n",
    "\n",
    "LightGBM is a gradient boosting framework that uses tree based learning algorithms.\n",
    "\n",
    "Pros:\n",
    "* fast\n",
    "* compatible with both small and large datasets\n",
    "* parallelizable\n",
    "* good handling of overfitting\n",
    "* **tt can be exported into an evalscript and visualized in eo-browser.**\n",
    "\n",
    "Cons:\n",
    "* doesn't take spatial context into account (here neural networks would help)\n",
    "* hard to interpret\n",
    "* already kind of a black box\n",
    "\n",
    "![](https://miro.medium.com/max/1000/1*qHbAsMNmdWQJkzm2SUA-8w.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can start with all available bands to see what we get. Later on we can adjust based on what we learned about the data / indices so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "clf = LGBMClassifier()  # check definition for changing parameters!\n",
    "clf.fit(features_train, labels_train, feature_name=feature_columns);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import apply_lgbm_to_eopatch\n",
    "\n",
    "# apply LGBM on eopatch\n",
    "eop = apply_lgbm_to_eopatch(\n",
    "    eopatch=eop,\n",
    "    timestamp_index=1,\n",
    "    classifier=clf,\n",
    "    output_mask_name=\"LGBM\",\n",
    ")\n",
    "\n",
    "# compare reference mask with our homecooked algorithm\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(12, 5))\n",
    "plot_mask(eop.mask_timeless[\"BURN_AREA\"], ax=axs[0], title=\"Reference mask\")\n",
    "plot_mask(\n",
    "    eop.mask_timeless[\"LGBM\"],\n",
    "    ax=axs[1],\n",
    "    title=\"LGBM Prediction\",\n",
    ")\n",
    "plot_mask(eop.data_timeless[\"LGBM_PROBA\"], ax=axs[2], title=\"LGBM Pseudo-Probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df[\"LGBM\"] = ...\n",
    "\n",
    "y_true = df[\"BURN_AREA\"].values\n",
    "y_pred = df[\"LGBM\"].values\n",
    "cm_lgbm = confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "plot_confusion_matrix(\n",
    "    conf_mat=cm_lgbm, show_normed=True, class_names=[\"Not burned\", \"Burned\"], axis=ax\n",
    ")\n",
    "ax.set_title(\"LGBM CM\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ROC curve is a receiver operating characteristic curve. The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings.\n",
    "\n",
    "It can be used to show the probabilities for the classifier to predict FP or FN cases at different thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# copy-pasta for convenience\n",
    "clf = LGBMClassifier()\n",
    "clf.fit(features_train, labels_train, feature_name=feature_columns)\n",
    "\n",
    "df[\"LGBM_PROBA\"] = ...  # hint: predict_proba\n",
    "\n",
    "y_true = df[\"BURN_AREA\"].values\n",
    "y_pred_proba = df[\"LGBM_PROBA\"].values\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.plot(fpr, tpr, label=\"ROC Curve LGBM\")\n",
    "\n",
    "# plot the homecooked point\n",
    "# fpr_homecooked = ...\n",
    "# tpr_homecooked = ...\n",
    "# ax.scatter(fpr_homecooked, tpr_homecooked, label=\"Homecooked Model\", color=\"C02\")\n",
    "\n",
    "# plot the decision-tree point\n",
    "# fpr_dt = ...\n",
    "# tpr_dt = ...\n",
    "# ax.scatter(fpr_dt, tpr_dt, label=\"Decision Trees Model\", color=\"C03\")\n",
    "\n",
    "# ideal point\n",
    "ax.scatter(0.0, 1.0, label=\"Ideal model\", color=\"black\", marker=\"X\")\n",
    "ax.set_xlabel(\"False Positive Rate (FPR)\")\n",
    "ax.set_ylabel(\"True Positive Rate(TPR)\")\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xlim([-0.005, 0.4])\n",
    "ax.set_ylim([0.6, 1.01]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Plot feature importance. Does it reflect what was seen in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "\n",
    "lightgbm.plot_importance(clf, figsize=(12, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export model to EO Browser\n",
    "\n",
    "LGBM is just a glorified decision tree. With some helper functions and black magic it's possible to convert it into an Evalscript and visualize it in the EO Browser. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "model_file = \"wildfires_example_model.pkl\"\n",
    "evalscript_file = \"wildfires_example_model_evalscript.js\"\n",
    "\n",
    "joblib.dump(clf, model_file)\n",
    "\n",
    "# convert and export to evalscript - bash command\n",
    "! python convert-model-to-custom-script.py --model $model_file --output $evalscript_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paste it to EO Browser!\n",
    "\n",
    "https://sentinelshare.page.link/g4vL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "What can you see?\n",
    "* Does the visualization reflect the quality as indicated by the scores?\n",
    "* Does it generalize well across different areas?\n",
    "* Does it generalize well across different years?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now it's time to play around and try to improve the model. Some ideas to get you started:\n",
    "* We made a random train/test split. Is this the best we can do?\n",
    "* Would adding indices (NDVI, NBR) to the features improve it?\n",
    "* We used default LGBM parameters, hyper parameter optimization could be one of the things to try.\n",
    "* Feel free to explore your own ideas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "9a2c73bfb89bda8307cf02b83c99d63a98e8f39629f267420571bf4772f20106"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
